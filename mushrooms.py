# -*- coding: utf-8 -*-
"""MUSHROOMS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/dylanthesweetpotato/c8836c3c8ca638ec5e771cd70f954cd8/mushrooms.ipynb
"""

import pandas as pd

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import sys
# 
# if 'google.colab' in sys.modules:
#     # Install packages in Colab
#     !pip install category_encoders==2.*
#     !pip install eli5
#     !pip install pandas-profiling==2.*
#     !pip install pdpbox
#     !pip install shap

"""Preprocessing and looking at the data"""

#from google.colab import files
#uploaded = files.upload()

col = ['poisonous','cap-shape','cap-surface','cap-color','bruises?','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type', 'spore-print-color', 'population', 'habitat']

url = ("https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data")
df= pd.read_csv(url, header=None, names= col)
df.head(10)

df.poisonous[df.poisonous == 'p'] = 1
df.poisonous[df.poisonous == 'e'] = 0
print(df)

df.head(2)

df.shape

#1 baselines.  poisonous  52% not poisonous and 48% poisonous
df['poisonous'].value_counts(normalize=True)

#check for nan values
df.isnull().sum().sort_values()

#  Lets split the data

from sklearn.model_selection import train_test_split
# Split train into train & val
train, test = train_test_split(df, train_size=0.80, test_size=0.20, 
                              stratify=df['poisonous'], random_state=42)

# Split train into train & val
train, val = train_test_split(train, train_size=0.80, test_size=0.20, 
                              stratify=train['poisonous'], random_state=42)

train.shape, test.shape, val.shape

train.head(2)

# drop some columns and only keep the easy ones to figure out for for people

#5 Use a scikit-learn pipeline to encode categoricals and fit a Decision Tree or Random Forest model.

# The status_group column is the target
target = 'poisonous'

# Get a dataframe with all train columns except the target
train_features = train.filter(['cap-shape','cap-color','population','habitat'])

# Get a list of the numeric features
numeric_features = train_features.select_dtypes(include='number').columns.tolist()

# Get a series with the cardinality of the nonnumeric features
cardinality = train_features.select_dtypes(exclude='number').nunique()

# Get a list of all categorical features with cardinality <= 50
categorical_features = cardinality[cardinality <= 100].index.tolist()

# Combine the lists 
features = numeric_features + categorical_features

X_train = train[features]
y_train = train[target]
X_val = val[features]
y_val = val[target]
X_test = test[features]
y_test = test[target]

y_train=y_train.astype('int')
y_val=y_val.astype('int')
y_test=y_test.astype('int')

X_train.head(3)

y_test.head(2)

y_val

#fitting pipeline
import category_encoders as ce 
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
    ce.OneHotEncoder(use_cat_names=True),
    SimpleImputer(strategy='mean'),
    RandomForestClassifier(n_jobs=-1, random_state=42)
)

# 6 get the vaidation accuracy

# valitdation for random forest
pipeline.fit(X_train, y_train)
print('Validation Accuracy:', pipeline.score(X_val, y_val))

from sklearn.metrics import accuracy_score
y_pred = pipeline.predict(X_test)

#MY ACCURACY SCORE FOR RANDOM FOREST
accuracy_score(y_test, y_pred)

#Use scikit-learn to fit a logistic regression model
import category_encoders as ce
from sklearn.linear_model import LogisticRegressionCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

encoder = ce.OneHotEncoder(use_cat_names=True)
X_train_encoded = encoder.fit_transform(X_train)
X_val_encoded = encoder.transform(X_val)
print(X_val_encoded.shape, X_val_encoded.shape)

imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train_encoded)
X_val_imputed = imputer.transform(X_val_encoded)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_val_scaled = scaler.transform(X_val_imputed)

model = LogisticRegressionCV(cv=5, n_jobs=-1, random_state=42)
model.fit(X_train_scaled, y_train)
print('Validation Accuracy', model.score(X_val_scaled, y_val))

# the logistic regression model is worse than the desicion tree model

#7 Get the test accuracy from the previous desicion tree,
from sklearn.metrics import accuracy_score
y_pred = pipeline.predict(X_test)

#MY ACCURACY SCORE FOR RANDOM FOREST
accuracy_score(y_test, y_pred)

from sklearn.model_selection import train_test_split
# Split train into train & val
train, test = train_test_split(df, train_size=0.80, test_size=0.20, 
                              stratify=df['poisonous'], random_state=42)

# Split train into train & val
train, val = train_test_split(train, train_size=0.80, test_size=0.20, 
                              stratify=train['poisonous'], random_state=42)

X_train2 = train[features]
y_train2 = train[target]
X_val2 = val[features]
y_val2 = val[target]
X_test2 = test[features]
y_test2 = test[target]

#Try a desicion tree
#Decision tree calculations
import category_encoders as ce
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeClassifier
pipeline1 = make_pipeline(
    ce.OrdinalEncoder(), 
    DecisionTreeClassifier(max_depth=8)
)

y_train2=y_train2.astype('int')
y_val2=y_val2.astype('int')
y_test2=y_test2.astype('int')

# validation score of decision tree

# Fit on train and score on val
pipeline1.fit(X_train2, y_train2)
print('Validation Accuracy:', pipeline1.score(X_val2, y_val2))

#MY ACCURACY SCORE FOR desicion tree
pipeline1.fit(X_train2, y_train2)
print('TEST Accuracy:', pipeline1.score(X_test2, y_test2))

#7 Get the test accuracy from the previous desicion tree,
from sklearn.metrics import accuracy_score
y_pred2 = pipeline1.predict(X_test2)

#MY ACCURACY SCORE FOR desicion tree
accuracy_score(y_test2, y_pred2)



from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

#  calculate permutation.
#It doesn't work with pipelines, so,
transformers = make_pipeline(
    ce.OrdinalEncoder(),
    SimpleImputer(strategy='median')
)

X_train_transformed = transformers.fit_transform(X_train)
X_val_transformed = transformers.fit_transform(X_val) 

model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(X_train_transformed, y_train)



#Permutation Importance
import eli5
from eli5.sklearn import PermutationImportance

#1. Calculate permutation importances
permuter = PermutationImportance(
    model, 
    scoring='accuracy',
    n_iter=5,
    random_state=42
)

permuter.fit(X_val_transformed, y_val)

feature_names = X_val.columns.tolist()
pd.Series(permuter.feature_importances_, feature_names).sort_values(ascending=False)

#Display permutation importances 
eli5.show_weights(
    permuter,
    top=None, #Shows all features
    feature_names=feature_names
)

# Visualize the decision tree
import graphviz
from sklearn.tree import export_graphviz

tree = pipeline1.named_steps['decisiontreeclassifier']

dot_data = export_graphviz(
    tree, 
    out_file=None, 
    feature_names=X_train.columns, 
    class_names=y_train.unique().astype(str), 
    filled=True, 
    impurity=False,
    proportion=True
)

graphviz.Source(dot_data)

# Commented out IPython magic to ensure Python compatibility.
# Get feature importances
rf = pipeline1.named_steps['decisiontreeclassifier']
importances = pd.Series(rf.feature_importances_, X_train.columns)

# Plot feature importances
# %matplotlib inline
import matplotlib.pyplot as plt

n = 20
plt.figure(figsize=(10,n/2))
plt.title(f'Top {n} features')
importances.sort_values()[-n:].plot.barh(color='grey');

import sklearn

from sklearn.metrics import plot_confusion_matrix

plot_confusion_matrix(pipeline1,
                      X_test, y_test,
                      values_format='.0f',
                      xticks_rotation='vertical',
                      cmap='Blues');